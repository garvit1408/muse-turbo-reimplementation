{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11295160,"sourceType":"datasetVersion","datasetId":7062751}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pickle\nimport pandas as pd\n\n# Define paths to data files and image directory\nIMAGE_FOLDER = \"/kaggle/input/kanakdb/images-20250405T093757Z-001/images\"\nTRAIN_TSV = \"/kaggle/input/kanakdb/train_df.tsv\"\nVAL_TSV = \"/kaggle/input/kanakdb/val_df.tsv\"\n\n# Read TSV data into DataFrames\ntrain_data = pd.read_csv(TRAIN_TSV, sep='\\t', header=None, names=[\"pid\", \"caption\", \"explanation\", \"target\"])\nval_data = pd.read_csv(VAL_TSV, sep='\\t', header=None, names=[\"pid\", \"caption\", \"explanation\", \"target\"])\n\nprint(f\"Number of training samples: {len(train_data)}, validation samples: {len(val_data)}\")\nprint(\"First training entry:\", train_data.iloc[0].to_dict())\n\n# Initialize variables for pickled objects and descriptions\nobjects_train = objects_val = descs_train = descs_val = None\n\n# File locations for pickled data\nOBJ_TRAIN_PATH = \"/kaggle/input/kanakdb/O_train.pkl\"\nDESC_TRAIN_PATH = \"/kaggle/input/kanakdb/D_train.pkl\"\nOBJ_VAL_PATH = \"/kaggle/input/kanakdb/O_val.pkl\"\nDESC_VAL_PATH = \"/kaggle/input/kanakdb/D_val.pkl\"\n\n# Load object annotations if available\nif os.path.exists(OBJ_TRAIN_PATH) and os.path.exists(OBJ_VAL_PATH):\n    with open(OBJ_TRAIN_PATH, 'rb') as f:\n        objects_train = pickle.load(f)\n    with open(OBJ_VAL_PATH, 'rb') as f:\n        objects_val = pickle.load(f)\n    print(\"Successfully loaded object annotations.\")\n\n# Load image descriptions if available\nif os.path.exists(DESC_TRAIN_PATH) and os.path.exists(DESC_VAL_PATH):\n    with open(DESC_TRAIN_PATH, 'rb') as f:\n        descs_train = pickle.load(f)\n    with open(DESC_VAL_PATH, 'rb') as f:\n        descs_val = pickle.load(f)\n    print(\"Successfully loaded image descriptions.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:17:07.102422Z","iopub.execute_input":"2025-04-06T13:17:07.102733Z","iopub.status.idle":"2025-04-06T13:17:07.154035Z","shell.execute_reply.started":"2025-04-06T13:17:07.102708Z","shell.execute_reply":"2025-04-06T13:17:07.153139Z"}},"outputs":[{"name":"stdout","text":"Number of training samples: 2984, validation samples: 176\nFirst training entry: {'pid': 'pid', 'caption': 'text', 'explanation': 'explanation', 'target': 'target_of_sarcasm'}\nSuccessfully loaded object annotations.\nSuccessfully loaded image descriptions.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom transformers import ViTFeatureExtractor, BartTokenizer\n\n# Initialize models\nVISION_MODEL = \"google/vit-base-patch16-224\"\nTEXT_MODEL = \"facebook/bart-base\"\n\nextractor = ViTFeatureExtractor.from_pretrained(VISION_MODEL)\ntokenizer = BartTokenizer.from_pretrained(TEXT_MODEL)\n\n# Token configuration\nMAX_INPUT_TOKENS = 64\nMAX_OUTPUT_TOKENS = 32\nBOS_ID = tokenizer.bos_token_id\nEOS_ID = tokenizer.eos_token_id\nPAD_ID = tokenizer.pad_token_id\n\nclass ImageTextDataset(Dataset):\n    def __init__(self, dataframe, object_map=None, description_map=None, mode=\"train\"):\n        self.dataframe = dataframe\n        self.objects = object_map\n        self.descriptions = description_map\n        self.mode = mode\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        row = self.dataframe.iloc[index]\n        pid = str(row[\"pid\"])\n        caption = str(row[\"caption\"])\n        target = str(row[\"target\"]) if pd.notna(row[\"target\"]) else \"\"\n        explanation = str(row[\"explanation\"]) if pd.notna(row[\"explanation\"]) else \"\"\n\n        # Attempt to load image; fallback to black placeholder\n        image_path = os.path.join(IMAGE_FOLDER, f\"{pid}.jpg\")\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except FileNotFoundError:\n            image = Image.new(\"RGB\", (224, 224), color=(0, 0, 0))\n\n        image_tensor = extractor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n\n        # Construct combined input text\n        components = [caption]\n        if self.objects and pid in self.objects:\n            obj_text = \" \".join(self.objects[pid]) if isinstance(self.objects[pid], list) else str(self.objects[pid])\n            components.append(obj_text)\n        if self.descriptions and pid in self.descriptions:\n            desc_text = \" \".join(self.descriptions[pid]) if isinstance(self.descriptions[pid], list) else str(self.descriptions[pid])\n            components.append(desc_text)\n        components.append(target)\n\n        separator = tokenizer.eos_token\n        combined_text = f\" {separator} \".join([part for part in components if part.strip()])\n        tokenized = tokenizer(combined_text, max_length=MAX_INPUT_TOKENS, truncation=True, padding=\"max_length\", add_special_tokens=False, return_tensors=\"pt\")\n\n        input_ids = tokenized[\"input_ids\"].squeeze(0)\n        attention = tokenized[\"attention_mask\"].squeeze(0)\n\n        # Ensure EOS token is present\n        token_count = attention.sum().item()\n        if token_count > 0:\n            last_index = int(token_count) - 1\n            if input_ids[last_index] != EOS_ID and last_index < MAX_INPUT_TOKENS - 1:\n                input_ids[last_index + 1] = EOS_ID\n                attention[last_index + 1] = 1\n            elif input_ids[last_index] != EOS_ID:\n                input_ids[last_index] = EOS_ID\n        else:\n            input_ids[0] = EOS_ID\n            attention[0] = 1\n\n        if self.mode != \"test\":\n            # Tokenize explanation for decoder input and labels\n            explanation_encoded = tokenizer(explanation, max_length=MAX_OUTPUT_TOKENS - 1, truncation=True, padding=\"max_length\", add_special_tokens=False, return_tensors=\"pt\")\n            explanation_ids = explanation_encoded[\"input_ids\"].squeeze(0)\n\n            decoder_inputs = torch.full((MAX_OUTPUT_TOKENS,), PAD_ID, dtype=torch.long)\n            decoder_inputs[0] = BOS_ID\n            decoder_inputs[1:] = explanation_ids[:MAX_OUTPUT_TOKENS - 1]\n\n            label_tokens = torch.full((MAX_OUTPUT_TOKENS,), PAD_ID, dtype=torch.long)\n            valid_tokens = (explanation_ids != PAD_ID).sum().item()\n            if valid_tokens > 0:\n                label_tokens[:valid_tokens] = explanation_ids[:valid_tokens]\n                if valid_tokens < MAX_OUTPUT_TOKENS:\n                    label_tokens[valid_tokens] = EOS_ID\n            else:\n                label_tokens[0] = EOS_ID\n        else:\n            decoder_inputs = None\n            label_tokens = None\n\n        return {\n            \"pixel_values\": image_tensor,\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention,\n            \"decoder_input_ids\": decoder_inputs,\n            \"labels\": label_tokens,\n            \"pid\": pid,\n            \"caption\": caption,\n            \"target\": target,\n            \"explanation\": explanation\n        }\n\n# Dataset instantiation\ntrain_dataset = ImageTextDataset(train_df, object_map=O_train, description_map=D_train, mode=\"train\")\nval_dataset = ImageTextDataset(val_df, object_map=O_val, description_map=D_val, mode=\"val\")\n\n# View a sample\nprint(\"Sample training datapoint:\", train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:20:22.823092Z","iopub.execute_input":"2025-04-06T13:20:22.823457Z","iopub.status.idle":"2025-04-06T13:20:23.855744Z","shell.execute_reply.started":"2025-04-06T13:20:22.823432Z","shell.execute_reply":"2025-04-06T13:20:23.854928Z"}},"outputs":[{"name":"stdout","text":"Sample training datapoint: {'pixel_values': tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n         [-1., -1., -1.,  ..., -1., -1., -1.],\n         [-1., -1., -1.,  ..., -1., -1., -1.],\n         ...,\n         [-1., -1., -1.,  ..., -1., -1., -1.],\n         [-1., -1., -1.,  ..., -1., -1., -1.],\n         [-1., -1., -1.,  ..., -1., -1., -1.]],\n\n        [[-1., -1., -1.,  ..., -1., -1., -1.],\n         [-1., -1., -1.,  ..., -1., -1., -1.],\n         [-1., -1., -1.,  ..., -1., -1., -1.],\n         ...,\n         [-1., -1., -1.,  ..., -1., -1., -1.],\n         [-1., -1., -1.,  ..., -1., -1., -1.],\n         [-1., -1., -1.,  ..., -1., -1., -1.]],\n\n        [[-1., -1., -1.,  ..., -1., -1., -1.],\n         [-1., -1., -1.,  ..., -1., -1., -1.],\n         [-1., -1., -1.,  ..., -1., -1., -1.],\n         ...,\n         [-1., -1., -1.,  ..., -1., -1., -1.],\n         [-1., -1., -1.,  ..., -1., -1., -1.],\n         [-1., -1., -1.,  ..., -1., -1., -1.]]]), 'input_ids': tensor([29015,  1437,     2,  1002,  1215,  1116,  1215,    29,  9636, 16836,\n            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'decoder_input_ids': tensor([    0,  3463, 11181,  1258,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1]), 'labels': tensor([ 3463, 11181,  1258,     2,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1]), 'pid': 'pid', 'caption': 'text', 'target': 'target_of_sarcasm', 'explanation': 'explanation'}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BartForConditionalGeneration, ViTModel\n\nclass FusionMultimodalModel(nn.Module):\n    def __init__(self, text_model=\"facebook/bart-base\", vision_model=\"google/vit-base-patch16-224-in21k\"):\n        super().__init__()\n\n        # Load pretrained vision and language encoders\n        self.vision_encoder = ViTModel.from_pretrained(vision_model)\n        self.language_model = BartForConditionalGeneration.from_pretrained(text_model)\n\n        # Optional: freeze visual backbone\n        for param in self.vision_encoder.parameters():\n            param.requires_grad = False\n\n        # Model dimensionality\n        hidden_dim = self.language_model.config.d_model\n\n        # Learnable gating for feature merging\n        self.gate_text = nn.Linear(2 * hidden_dim, hidden_dim)\n        self.gate_image = nn.Linear(2 * hidden_dim, hidden_dim)\n\n        # Padding token ID used in loss\n        self.pad_token_id = self.language_model.config.pad_token_id\n\n    def forward(self, input_ids, attention_mask, pixel_values, decoder_input_ids=None, labels=None):\n        # Encode textual tokens\n        lang_output = self.language_model.model.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True\n        )\n        H_text = lang_output.last_hidden_state  # (B, T, D)\n\n        # Encode image features\n        vis_output = self.vision_encoder(pixel_values=pixel_values, return_dict=True)\n        H_image = vis_output.last_hidden_state  # (B, V, D)\n\n        # Global summaries\n        global_image = H_image[:, 0, :]  # CLS token\n        mask_float = attention_mask.unsqueeze(-1).float()\n        global_text = (H_text * mask_float).sum(dim=1) / torch.clamp(mask_float.sum(dim=1), min=1e-9)\n\n        # Inter-modal conditioning\n        B, T, D = H_text.shape\n        V = H_image.shape[1]\n\n        global_image_rep = global_image.unsqueeze(1).expand(-1, T, -1)\n        global_text_rep = global_text.unsqueeze(1).expand(-1, V, -1)\n\n        conditioned_text = H_text * global_image_rep\n        conditioned_image = H_image * global_text_rep\n\n        # Gated multimodal fusion\n        text_merge = torch.cat([H_text, conditioned_text], dim=-1)\n        image_merge = torch.cat([H_image, conditioned_image], dim=-1)\n\n        gate_t = torch.sigmoid(self.gate_text(text_merge))\n        gate_v = torch.sigmoid(self.gate_image(image_merge))\n\n        fused_text = gate_t * H_text + (1 - gate_t) * conditioned_text\n        fused_image = gate_v * H_image + (1 - gate_v) * conditioned_image\n\n        # Combine both modalities\n        fused_sequence = torch.cat([fused_text, fused_image], dim=1)\n\n        # Create extended attention mask for decoder\n        mask_image = torch.ones((B, V), device=attention_mask.device).long()\n        combined_mask = torch.cat([attention_mask, mask_image], dim=1)\n\n        # Auto-generate decoder inputs if labels provided but decoder inputs aren't\n        if decoder_input_ids is None and labels is not None:\n            decoder_input_ids = self.language_model.prepare_decoder_input_ids_from_labels(labels)\n\n        # Run decoder with fused features\n        decoder_output = self.language_model.model.decoder(\n            input_ids=decoder_input_ids,\n            encoder_hidden_states=fused_sequence,\n            encoder_attention_mask=combined_mask,\n            return_dict=True,\n            use_cache=False\n        )\n        decoder_hidden = decoder_output.last_hidden_state\n\n        # Language modeling head\n        logits = self.language_model.lm_head(decoder_hidden) + self.language_model.final_logits_bias\n\n        # Loss calculation\n        loss = None\n        if labels is not None:\n            criterion = nn.CrossEntropyLoss(ignore_index=self.pad_token_id)\n            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n\n        return {\n            \"loss\": loss,\n            \"logits\": logits,\n            \"encoder_outputs\": fused_sequence\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:36:06.572205Z","iopub.execute_input":"2025-04-06T13:36:06.572545Z","iopub.status.idle":"2025-04-06T13:36:06.582902Z","shell.execute_reply.started":"2025-04-06T13:36:06.572516Z","shell.execute_reply":"2025-04-06T13:36:06.582004Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"!pip install evaluate\n!pip install rouge_score\n!pip install --upgrade nltk\n!pip install bert_score\nimport nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('wordnet', download_dir='/kaggle/working/nltk_data')\nnltk.download('omw-1.4', download_dir='/kaggle/working/nltk_data')\n\n# Telling nltk to look here\nimport os\nos.environ['NLTK_DATA'] = '/kaggle/working/nltk_data'\n!find /kaggle/working/nltk_data -type f\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:37:37.704641Z","iopub.execute_input":"2025-04-06T13:37:37.704975Z","iopub.status.idle":"2025-04-06T13:37:51.453444Z","shell.execute_reply.started":"2025-04-06T13:37:37.704951Z","shell.execute_reply":"2025-04-06T13:37:51.452391Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\nRequirement already satisfied: bert_score in /usr/local/lib/python3.10/dist-packages (0.3.13)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.47.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->bert_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.29.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->bert_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->bert_score) (2024.2.0)\n/kaggle/working/nltk_data/corpora/omw-1.4/msa/wn-data-zsm.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/msa/README\n/kaggle/working/nltk_data/corpora/omw-1.4/msa/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/msa/wn-data-ind.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/msa/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/hrv/README\n/kaggle/working/nltk_data/corpora/omw-1.4/hrv/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/hrv/wn-data-hrv.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/hrv/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/tha/wn-data-tha.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/tha/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/tha/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/slk/wn-data-lit.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/slk/wn-data-slk.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/slk/README\n/kaggle/working/nltk_data/corpora/omw-1.4/slk/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/slk/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/ita/wn-data-ita.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/ita/README\n/kaggle/working/nltk_data/corpora/omw-1.4/ita/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/ita/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/als/wn-data-als.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/als/README\n/kaggle/working/nltk_data/corpora/omw-1.4/als/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/als/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/iwn/wn-data-ita.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/iwn/README\n/kaggle/working/nltk_data/corpora/omw-1.4/iwn/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/iwn/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/dan/wn-data-dan.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/dan/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/dan/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/bul/README\n/kaggle/working/nltk_data/corpora/omw-1.4/bul/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/bul/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/bul/wn-data-bul.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/nld/wn-data-nld.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/nld/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/nld/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/por/README\n/kaggle/working/nltk_data/corpora/omw-1.4/por/wn-data-por.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/por/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/por/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/README\n/kaggle/working/nltk_data/corpora/omw-1.4/arb/wn-data-arb.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/arb/README\n/kaggle/working/nltk_data/corpora/omw-1.4/arb/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/arb/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/cow/wn-data-cmn.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/cow/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/cow/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/ell/wn-data-ell.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/ell/README\n/kaggle/working/nltk_data/corpora/omw-1.4/ell/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/swe/README\n/kaggle/working/nltk_data/corpora/omw-1.4/swe/wn-data-swe.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/swe/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/swe/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/pol/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/pol/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/pol/wn-data-pol.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/fin/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/fin/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/fin/wn-data-fin.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/heb/README\n/kaggle/working/nltk_data/corpora/omw-1.4/heb/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/heb/wn-data-heb.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/heb/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/jpn/wn-data-jpn.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/jpn/README\n/kaggle/working/nltk_data/corpora/omw-1.4/jpn/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/jpn/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/fra/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/fra/wn-data-fra.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/fra/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/isl/wn-data-isl.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/isl/README\n/kaggle/working/nltk_data/corpora/omw-1.4/isl/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/isl/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/slv/wn-data-slv.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/slv/README\n/kaggle/working/nltk_data/corpora/omw-1.4/slv/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/slv/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-glg.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-cat.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-spa.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-eus.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/mcr/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/mcr/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/nor/README\n/kaggle/working/nltk_data/corpora/omw-1.4/nor/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/nor/wn-data-nno.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/nor/wn-data-nob.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/nor/LICENSE\n/kaggle/working/nltk_data/corpora/omw-1.4/ron/wn-data-ron.tab\n/kaggle/working/nltk_data/corpora/omw-1.4/ron/README\n/kaggle/working/nltk_data/corpora/omw-1.4/ron/citation.bib\n/kaggle/working/nltk_data/corpora/omw-1.4/ron/LICENSE\n/kaggle/working/nltk_data/corpora/wordnet.zip\n/kaggle/working/nltk_data/corpora/omw-1.4.zip\n/kaggle/working/nltk_data/corpora/wordnet/index.adv\n/kaggle/working/nltk_data/corpora/wordnet/data.verb\n/kaggle/working/nltk_data/corpora/wordnet/cntlist.rev\n/kaggle/working/nltk_data/corpora/wordnet/data.adv\n/kaggle/working/nltk_data/corpora/wordnet/index.sense\n/kaggle/working/nltk_data/corpora/wordnet/data.adj\n/kaggle/working/nltk_data/corpora/wordnet/noun.exc\n/kaggle/working/nltk_data/corpora/wordnet/index.noun\n/kaggle/working/nltk_data/corpora/wordnet/README\n/kaggle/working/nltk_data/corpora/wordnet/lexnames\n/kaggle/working/nltk_data/corpora/wordnet/index.verb\n/kaggle/working/nltk_data/corpora/wordnet/citation.bib\n/kaggle/working/nltk_data/corpora/wordnet/adj.exc\n/kaggle/working/nltk_data/corpora/wordnet/index.adj\n/kaggle/working/nltk_data/corpora/wordnet/verb.exc\n/kaggle/working/nltk_data/corpora/wordnet/LICENSE\n/kaggle/working/nltk_data/corpora/wordnet/adv.exc\n/kaggle/working/nltk_data/corpora/wordnet/data.noun\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import os\nimport zipfile\nimport nltk\n\n# Define a custom location for NLTK resource loading\ncustom_nltk_dir = '/kaggle/working/nltk_data'\nos.makedirs(custom_nltk_dir, exist_ok=True)\n\n# Paths to zipped resources and their target folders\ncorpus_dir = os.path.join(custom_nltk_dir, 'corpora')\nwordnet_zip = os.path.join(corpus_dir, 'wordnet.zip')\nomw_zip = os.path.join(corpus_dir, 'omw-1.4.zip')\n\nwordnet_folder = os.path.join(corpus_dir, 'wordnet')\nomw_folder = os.path.join(corpus_dir, 'omw-1.4')\n\n# Unzip WordNet corpus if not already extracted\nif not os.path.exists(wordnet_folder):\n    with zipfile.ZipFile(wordnet_zip, 'r') as archive:\n        archive.extractall(corpus_dir)\n    print(\"WordNet corpus extracted.\")\n\n# Unzip OMW corpus if not already extracted\nif not os.path.exists(omw_folder):\n    with zipfile.ZipFile(omw_zip, 'r') as archive:\n        archive.extractall(corpus_dir)\n    print(\"OMW 1.4 corpus extracted.\")\n\n# Add the custom directory to NLTK's search paths\nif custom_nltk_dir not in nltk.data.path:\n    nltk.data.path.append(custom_nltk_dir)\n\n# Example METEOR scoring (commented for now)\n# meteor_score = meteor.compute(predictions=val_preds, references=val_refs)\n# print(\"METEOR score:\", meteor_score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:37:59.569942Z","iopub.execute_input":"2025-04-06T13:37:59.570281Z","iopub.status.idle":"2025-04-06T13:37:59.577062Z","shell.execute_reply.started":"2025-04-06T13:37:59.570251Z","shell.execute_reply":"2025-04-06T13:37:59.576141Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"--\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paste or import the TurboModel class before this point\n# Example:\nclass TurboModel(nn.Module):\n    def __init__(self, bart_model_name=\"facebook/bart-base\", vit_model_name=\"google/vit-base-patch16-224-in21k\"):\n        super(TurboModel, self).__init__()\n        self.vit = ViTModel.from_pretrained(vit_model_name)\n        self.bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name)\n\n        for param in self.vit.parameters():\n            param.requires_grad = False\n\n        d = self.bart_model.config.d_model\n        self.text_gate = nn.Linear(2 * d, d)\n        self.image_gate = nn.Linear(2 * d, d)\n        self.pad_token_id = self.bart_model.config.pad_token_id\n\n    def forward(self, input_ids, attention_mask, pixel_values, decoder_input_ids=None, labels=None):\n        encoder_outputs = self.bart_model.model.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True\n        )\n        A_t = encoder_outputs.last_hidden_state\n        A_v = self.vit(pixel_values=pixel_values, return_dict=True).last_hidden_state\n        E_v = A_v[:, 0, :]\n        mask = attention_mask.unsqueeze(-1).float()\n        text_len = mask.sum(dim=1)\n        E_t = (A_t * mask).sum(dim=1) / torch.clamp(text_len, min=1e-9)\n\n        B, T, d = A_t.shape\n        V = A_v.shape[1]\n        E_v_exp = E_v.unsqueeze(1).expand(-1, T, -1)\n        E_t_exp = E_t.unsqueeze(1).expand(-1, V, -1)\n        E_tv = A_t * E_v_exp\n        E_vt = A_v * E_t_exp\n\n        g_t = torch.sigmoid(self.text_gate(torch.cat([A_t, E_tv], dim=-1)))\n        g_v = torch.sigmoid(self.image_gate(torch.cat([A_v, E_vt], dim=-1)))\n        F_t = g_t * A_t + (1 - g_t) * E_tv\n        F_v = g_v * A_v + (1 - g_v) * E_vt\n\n        fused_hidden = torch.cat([F_t, F_v], dim=1)\n        fused_mask = torch.cat([attention_mask, torch.ones((B, V), device=attention_mask.device).long()], dim=1)\n\n        if decoder_input_ids is None and labels is not None:\n            decoder_input_ids = self.bart_model.prepare_decoder_input_ids_from_labels(labels)\n\n        decoder_outputs = self.bart_model.model.decoder(\n            input_ids=decoder_input_ids,\n            encoder_hidden_states=fused_hidden,\n            encoder_attention_mask=fused_mask,\n            use_cache=False,\n            return_dict=True\n        )\n        decoder_hidden = decoder_outputs.last_hidden_state\n        logits = self.bart_model.lm_head(decoder_hidden) + self.bart_model.final_logits_bias\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss(ignore_index=self.pad_token_id)\n            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n\n        return {\n            \"loss\": loss,\n            \"logits\": logits,\n            \"encoder_outputs\": fused_hidden\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:40:06.637933Z","iopub.execute_input":"2025-04-06T13:40:06.638257Z","iopub.status.idle":"2025-04-06T13:40:06.650010Z","shell.execute_reply.started":"2025-04-06T13:40:06.638235Z","shell.execute_reply":"2025-04-06T13:40:06.649065Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW, BartTokenizer\nfrom tqdm.auto import tqdm\nimport evaluate\n\n# Initialize tokenizer\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n\n# Training hyperparameters\nnum_epochs = 3\nbatch_size = 8\nlearning_rate = 1e-4\nmax_output_length = 64  # adjust if needed\nbos_token_id = tokenizer.bos_token_id\neos_token_id = tokenizer.eos_token_id\n\n# Data loaders (make sure `train_dataset` and `val_dataset` are defined)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)  # for generation convenience\n\n# Initialize model and optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TurboModel().to(device)\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\n# Evaluation metrics\nrouge = evaluate.load(\"rouge\")\nbleu = evaluate.load(\"bleu\")\nmeteor = evaluate.load(\"meteor\")\nbertscore = evaluate.load(\"bertscore\")\n\n# Training loop\nfor epoch in range(1, num_epochs + 1):\n    model.train()\n    total_loss = 0.0\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False)\n\n    for batch in progress_bar:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        decoder_input_ids = batch['decoder_input_ids'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values,\n            decoder_input_ids=decoder_input_ids,\n            labels=labels\n        )\n        loss = outputs['loss']\n        total_loss += loss.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        progress_bar.set_postfix({\"train_loss\": loss.item()})\n\n    avg_loss = total_loss / len(train_loader)\n\n    # Validation and generation\n    model.eval()\n    val_preds = []\n    val_refs = []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n\n            # Initialize generated_ids with BOS token\n            generated_ids = torch.tensor([[bos_token_id]], dtype=torch.long, device=device)\n\n            for _ in range(max_output_length):\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    pixel_values=pixel_values,\n                    decoder_input_ids=generated_ids\n                )\n                logits = outputs['logits']\n                next_token_id = torch.argmax(logits[0, -1, :]).unsqueeze(0).unsqueeze(0)\n                generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n\n                if next_token_id.item() == eos_token_id:\n                    break\n\n            # Decode generated tokens to string (omit BOS and EOS)\n            output_tokens = generated_ids[0].tolist()\n            if output_tokens and output_tokens[0] == bos_token_id:\n                output_tokens = output_tokens[1:]\n            if eos_token_id in output_tokens:\n                output_tokens = output_tokens[:output_tokens.index(eos_token_id)]\n\n            pred_text = tokenizer.decode(output_tokens, skip_special_tokens=True)\n            val_preds.append(pred_text.strip())\n\n            # Reference explanation\n            val_refs.append(batch['explanation'][0])\n\n    # Compute evaluation metrics\n    rouge_scores = rouge.compute(predictions=val_preds, references=val_refs)\n    bleu_scores = bleu.compute(predictions=val_preds, references=val_refs)\n    meteor_score = meteor.compute(predictions=val_preds, references=val_refs)\n    bert_score = bertscore.compute(predictions=val_preds, references=val_refs, lang=\"en\")\n\n    bleu1, bleu2, bleu3, bleu4 = bleu_scores['precisions']\n    bert_f1 = sum(bert_score['f1']) / len(bert_score['f1'])\n\n    # Print epoch summary\n    print(f\"\\nEpoch {epoch} Summary:\")\n    print(f\"  Training Loss: {avg_loss:.4f}\")\n    print(f\"  Validation ROUGE-1: {rouge_scores['rouge1']:.4f}, ROUGE-2: {rouge_scores['rouge2']:.4f}, ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n    print(f\"  Validation BLEU-1: {bleu1:.4f}, BLEU-2: {bleu2:.4f}, BLEU-3: {bleu3:.4f}, BLEU-4: {bleu4:.4f}\")\n    print(f\"  Validation METEOR: {meteor_score['meteor']:.4f}, BERTScore F1: {bert_f1:.4f}\")\n\n    # Save model checkpoint\n    ckpt_path = f\"turbo_model_epoch{epoch}.pt\"\n    torch.save(model.state_dict(), ckpt_path)\n    print(f\"  Saved model checkpoint to {ckpt_path}\")\n\n    # Show some validation samples\n    num_examples_to_show = 3\n    print(\"\\nExamples of generated explanations on validation set:\")\n    for i in range(min(num_examples_to_show, len(val_preds))):\n        print(f\"\\nExample {i + 1}:\")\n        # print(\"Caption:\", val_dataset.df.iloc[i][\"caption\"])\n        # print(\"Sarcasm Target:\", val_dataset.df.iloc[i][\"target\"])\n        print(\"Caption:\", val_dataset.dataframe.iloc[i][\"caption\"])\n        print(\"Sarcasm Target:\", val_dataset.dataframe.iloc[i][\"target\"])\n        print(\"Ground Truth Explanation:\", val_refs[i])\n        print(\"Generated Explanation:\", val_preds[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:47:44.072337Z","iopub.execute_input":"2025-04-06T13:47:44.072820Z","iopub.status.idle":"2025-04-06T13:57:05.336194Z","shell.execute_reply.started":"2025-04-06T13:47:44.072782Z","shell.execute_reply":"2025-04-06T13:57:05.335219Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Summary:\n  Training Loss: 2.4873\n  Validation ROUGE-1: 0.4922, ROUGE-2: 0.3401, ROUGE-L: 0.4636\n  Validation BLEU-1: 0.5924, BLEU-2: 0.4069, BLEU-3: 0.3058, BLEU-4: 0.2397\n  Validation METEOR: 0.4875, BERTScore F1: 0.9144\n  Saved model checkpoint to turbo_model_epoch1.pt\n\nExamples of generated explanations on validation set:\n\nExample 1:\nCaption: text\nSarcasm Target: target_of_sarcasm\nGround Truth Explanation: explanation\nGenerated Explanation: the target of the target_of_sarcasm isn't target.\n\nExample 2:\nCaption: '<user> thank u for this awesome network in malad ( see pic ) .  # patheticcs'\nSarcasm Target: <user>'s network in malad\nGround Truth Explanation: the author is pissed at <user> for not getting network in malad.\nGenerated Explanation: the author is pissed at <user> for such an awful network in malad.\n\nExample 3:\nCaption: Nothing like waiting for an hour on the tarmac for a gate to come open in snowy, windy Chicago!\nSarcasm Target: gate not opening \nGround Truth Explanation: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\nGenerated Explanation: the author is waiting for an hour on the tarmac for a gate to come open in snowy, windy Chicago.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 2 Summary:\n  Training Loss: 1.7092\n  Validation ROUGE-1: 0.4831, ROUGE-2: 0.3176, ROUGE-L: 0.4495\n  Validation BLEU-1: 0.5390, BLEU-2: 0.3518, BLEU-3: 0.2666, BLEU-4: 0.2063\n  Validation METEOR: 0.4958, BERTScore F1: 0.9112\n  Saved model checkpoint to turbo_model_epoch2.pt\n\nExamples of generated explanations on validation set:\n\nExample 1:\nCaption: text\nSarcasm Target: target_of_sarcasm\nGround Truth Explanation: explanation\nGenerated Explanation: the target of the author'sarcasm isn't the target.\n\nExample 2:\nCaption: '<user> thank u for this awesome network in malad ( see pic ) .  # patheticcs'\nSarcasm Target: <user>'s network in malad\nGround Truth Explanation: the author is pissed at <user> for not getting network in malad.\nGenerated Explanation: the author is pissed at <user> for such an awful network in malad.\n\nExample 3:\nCaption: Nothing like waiting for an hour on the tarmac for a gate to come open in snowy, windy Chicago!\nSarcasm Target: gate not opening \nGround Truth Explanation: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\nGenerated Explanation: the author hates waiting for an hour on the tarmac for a gate to come open in snowy, windy Chicago.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nEpoch 3 Summary:\n  Training Loss: 1.3186\n  Validation ROUGE-1: 0.5166, ROUGE-2: 0.3572, ROUGE-L: 0.4906\n  Validation BLEU-1: 0.5885, BLEU-2: 0.4035, BLEU-3: 0.3120, BLEU-4: 0.2435\n  Validation METEOR: 0.5236, BERTScore F1: 0.9164\n  Saved model checkpoint to turbo_model_epoch3.pt\n\nExamples of generated explanations on validation set:\n\nExample 1:\nCaption: text\nSarcasm Target: target_of_sarcasm\nGround Truth Explanation: explanation\nGenerated Explanation: the author doesn't like the target of the attack.\n\nExample 2:\nCaption: '<user> thank u for this awesome network in malad ( see pic ) .  # patheticcs'\nSarcasm Target: <user>'s network in malad\nGround Truth Explanation: the author is pissed at <user> for not getting network in malad.\nGenerated Explanation: the author is pissed at <user> for such poor network in malad.\n\nExample 3:\nCaption: Nothing like waiting for an hour on the tarmac for a gate to come open in snowy, windy Chicago!\nSarcasm Target: gate not opening \nGround Truth Explanation: nothing worst than waiting for an hour on the tarmac for a gate to come open in snowy, windy chicago.\nGenerated Explanation: the author hates waiting for an hour on tarmac for a gate to come open in snowy, windy Chicago.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"Generate Test data ","metadata":{}},{"cell_type":"code","source":"# Switch to evaluation mode and generate for test set\nmodel.eval()\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\ntest_preds = []\ntest_ids = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Explanations\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        pid = batch['pid'][0]\n\n        # Start generation with BOS token\n        generated_ids = torch.tensor([[bos_token_id]], dtype=torch.long, device=device)\n\n        for _ in range(max_output_length):\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                pixel_values=pixel_values,\n                decoder_input_ids=generated_ids\n            )\n            logits = outputs['logits']\n            next_token_id = torch.argmax(logits[0, -1, :]).unsqueeze(0).unsqueeze(0)\n            generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n\n            if next_token_id.item() == eos_token_id:\n                break\n\n        # Decode generated sequence\n        output_tokens = generated_ids[0].tolist()\n\n        # Remove BOS if present\n        if output_tokens and output_tokens[0] == bos_token_id:\n            output_tokens = output_tokens[1:]\n\n        # Truncate at EOS if exists\n        if eos_token_id in output_tokens:\n            output_tokens = output_tokens[:output_tokens.index(eos_token_id)]\n\n        # Decode to text\n        expl = tokenizer.decode(output_tokens, skip_special_tokens=True).strip()\n        test_preds.append(expl)\n        test_ids.append(pid)\n\n# Save the generated explanations to a TSV file\noutput_file = \"generated_test_explanations.tsv\"\nwith open(output_file, \"w\", encoding=\"utf-8\") as f:\n    for pid, expl in zip(test_ids, test_preds):\n        f.write(f\"{pid}\\t{expl}\\n\")\n\nprint(f\"\\nSaved generated test explanations to {output_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:17:00.842926Z","iopub.execute_input":"2025-04-06T14:17:00.843238Z","iopub.status.idle":"2025-04-06T14:17:01.756810Z","shell.execute_reply.started":"2025-04-06T14:17:00.843215Z","shell.execute_reply":"2025-04-06T14:17:01.755915Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating Test Explanations:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d08977f58a66458889a176a390647f5e"}},"metadata":{}},{"name":"stdout","text":"\nSaved generated test explanations to val_df.tsv\n","output_type":"stream"}],"execution_count":34}]}